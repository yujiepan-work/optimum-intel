{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eaed3927-e315-46d3-8889-df3f3bbcbf6b",
   "metadata": {},
   "source": [
    "# Jointly prune, quantize and distill a Hugging Face Text Classification Model with OpenVINO\n",
    "\n",
    "This notebook shows how to quantize a text classification model with OpenVINO's [Neural Network Compression Framework](https://github.com/openvinotoolkit/nncf) (NNCF). \n",
    "\n",
    "TODO\n",
    "\n",
    "With quantization, we reduce the precision of the model's weights and activations from floating point (FP32) to integer (INT8). This results in a smaller model with faster inference times with OpenVINO Runtime. \n",
    "\n",
    "The notebook demonstrates post-training quantization, which does not require specific hardware to execute. A laptop or desktop with a recent Intel Core processor is recommended for best results. To install the requirements for this notebook, please do `pip install -r requirements.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0407fc92-c052-47b7-8721-01836adf3b54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-04T14:40:23.169301Z",
     "iopub.status.busy": "2022-12-04T14:40:23.169096Z",
     "iopub.status.idle": "2022-12-04T14:40:25.146758Z",
     "shell.execute_reply": "2022-12-04T14:40:25.146281Z",
     "shell.execute_reply.started": "2022-12-04T14:40:23.169248Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:NNCF initialized successfully. Supported frameworks detected: torch, onnx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "import datasets\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from evaluate import evaluator\n",
    "from openvino.runtime import Core\n",
    "from optimum.intel.openvino import OVModelForSequenceClassification, OVQuantizer, OVTrainer, OVConfig, OVTrainingArguments\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline, AutoConfig\n",
    "\n",
    "transformers.logging.set_verbosity_error()\n",
    "datasets.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124bd9ad-077c-4f41-b579-0bf978fe6a1e",
   "metadata": {},
   "source": [
    "## Preview the Dataset\n",
    "\n",
    "The `datasets` library makes it easy to load datasets. Common datasets can be loaded from the Hugging Face Hub by providing the name of the dataset. See https://github.com/huggingface/datasets. We can load the SQuAD dataset with `load_dataset` and show a random dataset item. Every dataset item in the SQuAD dataset has a unique id, a title which denotes the category, a context and a question, and answers. The answer is a subset of the context, and both the text of the answer, and the start position of the answer in the context (`answer_start`) are returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "602fe46f-c96a-4a0f-9338-58339d466f3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-04T14:40:27.656861Z",
     "iopub.status.busy": "2022-12-04T14:40:27.656663Z",
     "iopub.status.idle": "2022-12-04T14:40:31.357513Z",
     "shell.execute_reply": "2022-12-04T14:40:31.357057Z",
     "shell.execute_reply.started": "2022-12-04T14:40:27.656848Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c1723abafc84ed38e84241e687b4653",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TASK_NAME = \"sst2\"\n",
    "dataset = datasets.load_dataset(\"glue\", TASK_NAME)\n",
    "metric = evaluate.load('glue', TASK_NAME)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "889a16fe-2bc0-477e-b8d6-02a4f7508f03",
   "metadata": {},
   "source": [
    "## Settings\n",
    "\n",
    "We define MODEL_ID and DATASET_NAME, and the paths for the quantized model file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c32f9a76-414b-43d9-9769-af131223f1c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-04T14:40:25.147946Z",
     "iopub.status.busy": "2022-12-04T14:40:25.147671Z",
     "iopub.status.idle": "2022-12-04T14:40:25.150427Z",
     "shell.execute_reply": "2022-12-04T14:40:25.150085Z",
     "shell.execute_reply.started": "2022-12-04T14:40:25.147933Z"
    }
   },
   "outputs": [],
   "source": [
    "MODEL_ID = \"textattack/bert-base-uncased-SST-2\"\n",
    "TEACHER_MODEL_ID = 'TehranNLP-org/bert-large-sst2'\n",
    "base_model_path = Path(f\"/tmp/yujiepan/optimum-notebook/{MODEL_ID}\")\n",
    "fp32_model_path = base_model_path.with_name(base_model_path.name + \"_FP32\")\n",
    "jpqd_model_path = base_model_path.with_name(base_model_path.name + \"_JPQD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3547df-5603-41b4-8752-df5fb7347be0",
   "metadata": {},
   "source": [
    "## Load Model and Tokenizer\n",
    "\n",
    "We load the model from the Hugging Face Hub. The model will be automatically downloaded if it has not been downloaded before, or loaded from the cache otherwise.\n",
    "\n",
    "We also load the tokenizer, which converts the questions and contexts from the dataset to tokens, converting the inputs in a format the model expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38641b14-07d0-49d5-af86-8b5247ae39d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-04T14:40:25.151098Z",
     "iopub.status.busy": "2022-12-04T14:40:25.150929Z",
     "iopub.status.idle": "2022-12-04T14:40:27.655923Z",
     "shell.execute_reply": "2022-12-04T14:40:27.655188Z",
     "shell.execute_reply.started": "2022-12-04T14:40:25.151086Z"
    }
   },
   "outputs": [],
   "source": [
    "labels = dataset['train'].features['label'].names\n",
    "num_labels = len(labels)\n",
    "id2label = dict(zip(range(num_labels), labels))\n",
    "label2id = dict(zip(labels, range(num_labels)))\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "model.save_pretrained(fp32_model_path)\n",
    "teacher_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    TEACHER_MODEL_ID,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fb15b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 7592, 2088, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "from contextlib import contextmanager\n",
    "from unittest.mock import patch\n",
    "\n",
    "@contextmanager\n",
    "def patch_tokenizer():\n",
    "    _original_call = tokenizer.__class__.__call__\n",
    "\n",
    "    def _new_call(self, *args, **kwargs):\n",
    "        kwargs['max_length'] = 128\n",
    "        kwargs['padding'] = 'max_length'\n",
    "        kwargs['truncation'] = True\n",
    "        return _original_call(self, *args, **kwargs)\n",
    "\n",
    "    with patch('.'.join([_original_call.__module__, _original_call.__qualname__]), _new_call):\n",
    "        yield\n",
    "\n",
    "\n",
    "with patch_tokenizer():\n",
    "    print(tokenizer(\"Hello world\"))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce4dfc2d-f007-4455-8043-edc5468b87e2",
   "metadata": {},
   "source": [
    "## Joint pruning, quantization and distlllation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5483309f-0b12-4c10-ab0b-58c50981f495",
   "metadata": {
    "tags": []
   },
   "source": [
    "For post-training quantization (PTQ), we first start by loading the model using the `AutoModelForQuestionAnswering` class. After instantiating an `OVQuantizer`, we need to provide a dataset for the calibration step. You can apply quantization on your model by calling the `quantize` method. That's all!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9b14899a-3aec-46b6-9d4b-332452e9cb25",
   "metadata": {},
   "source": [
    "### Preprocess the Dataset\n",
    "\n",
    "We need a representative calibration dataset to quantize the model. The SQuAD dataset is pretrained on a large dataset with a wide variety of questions and answers, and it generalizes pretty well to questions and contexts it has never seen before. For production use, you would finetune this dataset with questions and context specific to your domain. In this notebook, we use a subset of the SQuAD dataset, for demonstration purposes. We chose the _Super Bowl 50_ category from the validation subset of SQuAD because it has a large number of questions.\n",
    "\n",
    "Post-training quantization does not need a training and validation dataset, because we will not train the model, but we define these splits here to use a training split for calibration, and a validation split for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8be68958-ce5e-4cc6-b8e7-2867feaf084b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-04T14:40:31.358230Z",
     "iopub.status.busy": "2022-12-04T14:40:31.358053Z",
     "iopub.status.idle": "2022-12-04T14:40:31.360666Z",
     "shell.execute_reply": "2022-12-04T14:40:31.360301Z",
     "shell.execute_reply.started": "2022-12-04T14:40:31.358218Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1840c4faf64e47d29761d10b97c4431a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/68 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abc59a8dc57e49edabe751138cbf4bc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b759d29dba3740c797f0ab8383db8fa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'sentence': ['hide new secretions from the parental units ',\n",
       "  'contains no wit , only labored gags ',\n",
       "  'that loves its characters and communicates something rather beautiful about human nature ',\n",
       "  'remains utterly satisfied to remain the same throughout ',\n",
       "  'on the worst revenge-of-the-nerds clichÃ©s the filmmakers could dredge up '],\n",
       " 'label': [0, 0, 1, 0, 0],\n",
       " 'idx': [0, 1, 2, 3, 4],\n",
       " 'input_ids': [[101,\n",
       "   5342,\n",
       "   2047,\n",
       "   3595,\n",
       "   8496,\n",
       "   2013,\n",
       "   1996,\n",
       "   18643,\n",
       "   3197,\n",
       "   102,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [101,\n",
       "   3397,\n",
       "   2053,\n",
       "   15966,\n",
       "   1010,\n",
       "   2069,\n",
       "   4450,\n",
       "   2098,\n",
       "   18201,\n",
       "   2015,\n",
       "   102,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [101,\n",
       "   2008,\n",
       "   7459,\n",
       "   2049,\n",
       "   3494,\n",
       "   1998,\n",
       "   10639,\n",
       "   2015,\n",
       "   2242,\n",
       "   2738,\n",
       "   3376,\n",
       "   2055,\n",
       "   2529,\n",
       "   3267,\n",
       "   102,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [101,\n",
       "   3464,\n",
       "   12580,\n",
       "   8510,\n",
       "   2000,\n",
       "   3961,\n",
       "   1996,\n",
       "   2168,\n",
       "   2802,\n",
       "   102,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [101,\n",
       "   2006,\n",
       "   1996,\n",
       "   5409,\n",
       "   7195,\n",
       "   1011,\n",
       "   1997,\n",
       "   1011,\n",
       "   1996,\n",
       "   1011,\n",
       "   11265,\n",
       "   17811,\n",
       "   18856,\n",
       "   17322,\n",
       "   2015,\n",
       "   1996,\n",
       "   16587,\n",
       "   2071,\n",
       "   2852,\n",
       "   24225,\n",
       "   2039,\n",
       "   102,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0]],\n",
       " 'token_type_ids': [[0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0]],\n",
       " 'attention_mask': [[1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0]]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_to_keys = {\n",
    "    \"cola\": (\"sentence\", None),\n",
    "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "    \"mnli-mm\": (\"premise\", \"hypothesis\"),\n",
    "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "    \"qnli\": (\"question\", \"sentence\"),\n",
    "    \"qqp\": (\"question1\", \"question2\"),\n",
    "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
    "    \"sst2\": (\"sentence\", None),\n",
    "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
    "    \"wnli\": (\"sentence1\", \"sentence2\"),\n",
    "}\n",
    "\n",
    "@patch_tokenizer()\n",
    "def preprocess_fn(examples):\n",
    "    sentence1_key, sentence2_key = task_to_keys[TASK_NAME]\n",
    "    if sentence2_key is None:\n",
    "        return tokenizer(examples[sentence1_key])\n",
    "    return tokenizer(examples[sentence1_key], examples[sentence2_key], truncation=True)\n",
    "\n",
    "encoded_dataset = dataset.map(preprocess_fn, batched=True)\n",
    "encoded_dataset['train'][:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "96276c57",
   "metadata": {},
   "source": [
    "### Define the compression config\n",
    "Please see NNCF...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef976ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_config = [\n",
    "    {\n",
    "        \"algorithm\": \"movement_sparsity\",\n",
    "        \"params\": {\n",
    "            \"warmup_start_epoch\": 1,\n",
    "            \"warmup_end_epoch\": 2,\n",
    "            \"importance_regularization_factor\": 0.05,\n",
    "            \"enable_structured_masking\": True,\n",
    "        },\n",
    "        \"sparse_structure_by_scopes\": [\n",
    "            {\"mode\": \"block\", \"sparse_factors\": [32, 32], \"target_scopes\": \"{re}.*BertAttention.*\"},\n",
    "            {\"mode\": \"per_dim\", \"axis\": 0, \"target_scopes\": \"{re}.*BertIntermediate.*\"},\n",
    "            {\"mode\": \"per_dim\", \"axis\": 1, \"target_scopes\": \"{re}.*BertOutput.*\"},\n",
    "        ],\n",
    "        \"ignored_scopes\": [\n",
    "            \"{re}.*NNCFEmbedding.*\",\n",
    "            \"{re}.*LayerNorm.*\",\n",
    "            \"{re}.*pooler.*\",\n",
    "            \"{re}.*classifier.*\"]\n",
    "    },\n",
    "    {\n",
    "        \"algorithm\": \"quantization\",\n",
    "        \"preset\": \"mixed\",\n",
    "        \"overflow_fix\": \"disable\",\n",
    "        \"initializer\": {\n",
    "            \"range\": {\n",
    "                \"num_init_samples\": 32,\n",
    "                \"type\": \"percentile\",\n",
    "                \"params\":\n",
    "                {\n",
    "                    \"min_percentile\": 0.01,\n",
    "                    \"max_percentile\": 99.99\n",
    "                }\n",
    "            },\n",
    "            \"batchnorm_adaptation\": {\n",
    "                \"num_bn_adaptation_samples\": 32\n",
    "            }\n",
    "        },\n",
    "        \"scope_overrides\": {\"activations\": {\"{re}.*matmul_0\": {\"mode\": \"symmetric\"}}},\n",
    "        \"ignored_scopes\": [\n",
    "            \"{re}.*__add___[0-1]\",\n",
    "            \"{re}.*layer_norm_0\",\n",
    "            \"{re}.*matmul_1\",\n",
    "            \"{re}.*__truediv__*\",\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "ov_config = OVConfig(compression=compression_config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eb0b0738-fdc9-4557-97bf-b4c6709280cc",
   "metadata": {},
   "source": [
    "### Distiil the Model with JPQD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12a2cb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size= 64\n",
    "learning_rate = 2e-5\n",
    "num_train_epochs = 3\n",
    "\n",
    "args = OVTrainingArguments(\n",
    "    jpqd_model_path,\n",
    "    do_train=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=learning_rate,\n",
    "    optim='adamw_torch',\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    push_to_hub=False,\n",
    "    report_to='none'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5c9f5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "36d905eb",
   "metadata": {},
   "source": [
    "Initialize the `OVTrainer`. During the initialization, we will modify the model to supportquantization. So iwll be a bit slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33ae4557",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "trainer = OVTrainer(\n",
    "    model,\n",
    "    teacher_model=teacher_model,\n",
    "    args=args,\n",
    "    train_dataset=encoded_dataset[\"train\"].select(range(512)),\n",
    "    eval_dataset=encoded_dataset[\"validation\"].select(range(512)),\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    ov_config=ov_config,\n",
    "    feature='text-classification'\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3afcdcb1",
   "metadata": {},
   "source": [
    "Train and save the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9364cca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `NNCFNetwork.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `NNCFNetwork.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 512\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 24\n",
      "The following columns in the evaluation set don't have a corresponding argument in `NNCFNetwork.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `NNCFNetwork.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 512\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to /tmp/yujiepan/optimum-notebook/textattack/bert-base-uncased-SST-2_JPQD/checkpoint-8\n",
      "Configuration saved in /tmp/yujiepan/optimum-notebook/textattack/bert-base-uncased-SST-2_JPQD/checkpoint-8/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2939246892929077, 'eval_accuracy': 0.912109375, 'eval_runtime': 7.5664, 'eval_samples_per_second': 67.668, 'eval_steps_per_second': 1.057, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in /tmp/yujiepan/optimum-notebook/textattack/bert-base-uncased-SST-2_JPQD/checkpoint-8/pytorch_model.bin\n",
      "tokenizer config file saved in /tmp/yujiepan/optimum-notebook/textattack/bert-base-uncased-SST-2_JPQD/checkpoint-8/tokenizer_config.json\n",
      "Special tokens file saved in /tmp/yujiepan/optimum-notebook/textattack/bert-base-uncased-SST-2_JPQD/checkpoint-8/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nvme2/yujiepan/tools/miniconda3/envs/jpqd-optimum-postpr/lib/python3.8/site-packages/nncf/torch/quantization/quantize_functions.py:141: FutureWarning: 'torch.onnx._patch_torch._graph_op' is deprecated in version 1.13 and will be removed in version 1.14. Please note 'g.op()' is to be removed from torch.Graph. Please open a GitHub issue if you need this functionality..\n",
      "  output = g.op(\n",
      "/nvme2/yujiepan/tools/miniconda3/envs/jpqd-optimum-postpr/lib/python3.8/site-packages/torch/onnx/_patch_torch.py:81: UserWarning: The shape inference of org.openvinotoolkit::FakeQuantize type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)\n",
      "  _C._jit_pass_onnx_node_shape_type_inference(\n",
      "/nvme2/yujiepan/tools/miniconda3/envs/jpqd-optimum-postpr/lib/python3.8/site-packages/torch/onnx/utils.py:687: UserWarning: The shape inference of org.openvinotoolkit::FakeQuantize type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n",
      "/nvme2/yujiepan/tools/miniconda3/envs/jpqd-optimum-postpr/lib/python3.8/site-packages/torch/onnx/utils.py:1178: UserWarning: The shape inference of org.openvinotoolkit::FakeQuantize type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `NNCFNetwork.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `NNCFNetwork.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 512\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to /tmp/yujiepan/optimum-notebook/textattack/bert-base-uncased-SST-2_JPQD/checkpoint-16\n",
      "Configuration saved in /tmp/yujiepan/optimum-notebook/textattack/bert-base-uncased-SST-2_JPQD/checkpoint-16/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 14.533432006835938, 'eval_accuracy': 0.75390625, 'eval_runtime': 7.647, 'eval_samples_per_second': 66.955, 'eval_steps_per_second': 1.046, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in /tmp/yujiepan/optimum-notebook/textattack/bert-base-uncased-SST-2_JPQD/checkpoint-16/pytorch_model.bin\n",
      "tokenizer config file saved in /tmp/yujiepan/optimum-notebook/textattack/bert-base-uncased-SST-2_JPQD/checkpoint-16/tokenizer_config.json\n",
      "Special tokens file saved in /tmp/yujiepan/optimum-notebook/textattack/bert-base-uncased-SST-2_JPQD/checkpoint-16/special_tokens_map.json\n",
      "/nvme2/yujiepan/tools/miniconda3/envs/jpqd-optimum-postpr/lib/python3.8/site-packages/nncf/torch/quantization/quantize_functions.py:141: FutureWarning: 'torch.onnx._patch_torch._graph_op' is deprecated in version 1.13 and will be removed in version 1.14. Please note 'g.op()' is to be removed from torch.Graph. Please open a GitHub issue if you need this functionality..\n",
      "  output = g.op(\n",
      "/nvme2/yujiepan/tools/miniconda3/envs/jpqd-optimum-postpr/lib/python3.8/site-packages/torch/onnx/_patch_torch.py:81: UserWarning: The shape inference of org.openvinotoolkit::FakeQuantize type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)\n",
      "  _C._jit_pass_onnx_node_shape_type_inference(\n",
      "/nvme2/yujiepan/tools/miniconda3/envs/jpqd-optimum-postpr/lib/python3.8/site-packages/torch/onnx/utils.py:687: UserWarning: The shape inference of org.openvinotoolkit::FakeQuantize type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n",
      "/nvme2/yujiepan/tools/miniconda3/envs/jpqd-optimum-postpr/lib/python3.8/site-packages/torch/onnx/utils.py:1178: UserWarning: The shape inference of org.openvinotoolkit::FakeQuantize type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `NNCFNetwork.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `NNCFNetwork.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 512\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to /tmp/yujiepan/optimum-notebook/textattack/bert-base-uncased-SST-2_JPQD/checkpoint-24\n",
      "Configuration saved in /tmp/yujiepan/optimum-notebook/textattack/bert-base-uncased-SST-2_JPQD/checkpoint-24/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.261968195438385, 'eval_accuracy': 0.916015625, 'eval_runtime': 7.6717, 'eval_samples_per_second': 66.738, 'eval_steps_per_second': 1.043, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in /tmp/yujiepan/optimum-notebook/textattack/bert-base-uncased-SST-2_JPQD/checkpoint-24/pytorch_model.bin\n",
      "tokenizer config file saved in /tmp/yujiepan/optimum-notebook/textattack/bert-base-uncased-SST-2_JPQD/checkpoint-24/tokenizer_config.json\n",
      "Special tokens file saved in /tmp/yujiepan/optimum-notebook/textattack/bert-base-uncased-SST-2_JPQD/checkpoint-24/special_tokens_map.json\n",
      "/nvme2/yujiepan/tools/miniconda3/envs/jpqd-optimum-postpr/lib/python3.8/site-packages/nncf/torch/quantization/quantize_functions.py:141: FutureWarning: 'torch.onnx._patch_torch._graph_op' is deprecated in version 1.13 and will be removed in version 1.14. Please note 'g.op()' is to be removed from torch.Graph. Please open a GitHub issue if you need this functionality..\n",
      "  output = g.op(\n",
      "/nvme2/yujiepan/tools/miniconda3/envs/jpqd-optimum-postpr/lib/python3.8/site-packages/torch/onnx/_patch_torch.py:81: UserWarning: The shape inference of org.openvinotoolkit::FakeQuantize type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)\n",
      "  _C._jit_pass_onnx_node_shape_type_inference(\n",
      "/nvme2/yujiepan/tools/miniconda3/envs/jpqd-optimum-postpr/lib/python3.8/site-packages/torch/onnx/utils.py:687: UserWarning: The shape inference of org.openvinotoolkit::FakeQuantize type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n",
      "/nvme2/yujiepan/tools/miniconda3/envs/jpqd-optimum-postpr/lib/python3.8/site-packages/torch/onnx/utils.py:1178: UserWarning: The shape inference of org.openvinotoolkit::FakeQuantize type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to /tmp/yujiepan/optimum-notebook/textattack/bert-base-uncased-SST-2_JPQD\n",
      "Configuration saved in /tmp/yujiepan/optimum-notebook/textattack/bert-base-uncased-SST-2_JPQD/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 123.8275, 'train_samples_per_second': 12.404, 'train_steps_per_second': 0.194, 'train_loss': 3.4295008977254233, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in /tmp/yujiepan/optimum-notebook/textattack/bert-base-uncased-SST-2_JPQD/pytorch_model.bin\n",
      "tokenizer config file saved in /tmp/yujiepan/optimum-notebook/textattack/bert-base-uncased-SST-2_JPQD/tokenizer_config.json\n",
      "Special tokens file saved in /tmp/yujiepan/optimum-notebook/textattack/bert-base-uncased-SST-2_JPQD/special_tokens_map.json\n",
      "/nvme2/yujiepan/tools/miniconda3/envs/jpqd-optimum-postpr/lib/python3.8/site-packages/nncf/torch/quantization/quantize_functions.py:141: FutureWarning: 'torch.onnx._patch_torch._graph_op' is deprecated in version 1.13 and will be removed in version 1.14. Please note 'g.op()' is to be removed from torch.Graph. Please open a GitHub issue if you need this functionality..\n",
      "  output = g.op(\n",
      "/nvme2/yujiepan/tools/miniconda3/envs/jpqd-optimum-postpr/lib/python3.8/site-packages/torch/onnx/_patch_torch.py:81: UserWarning: The shape inference of org.openvinotoolkit::FakeQuantize type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)\n",
      "  _C._jit_pass_onnx_node_shape_type_inference(\n",
      "/nvme2/yujiepan/tools/miniconda3/envs/jpqd-optimum-postpr/lib/python3.8/site-packages/torch/onnx/utils.py:687: UserWarning: The shape inference of org.openvinotoolkit::FakeQuantize type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n",
      "/nvme2/yujiepan/tools/miniconda3/envs/jpqd-optimum-postpr/lib/python3.8/site-packages/torch/onnx/utils.py:1178: UserWarning: The shape inference of org.openvinotoolkit::FakeQuantize type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.save_model()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2574cc63-aad3-4c28-aa6f-e553de911ce5",
   "metadata": {},
   "source": [
    "## Compare FP32 model with optimized model by JPQD\n",
    "\n",
    "We compare the accuracy, model size and inference results and latency of the FP32 and INT8 models.\n",
    "### Inference Pipeline\n",
    "\n",
    "Transformers [Pipelines](https://huggingface.co/docs/transformers/main/en/pipeline_tutorial) simplify model inference. A `Pipeline` is created by adding a task, model and tokenizer to the `pipeline` function. Inference is then as simple as `qa_pipeline({\"question\": question, \"context\": context})`.\n",
    "\n",
    "We create two pipelines: `hf_qa_pipeline` and `ov_qa_pipeline_ptq` to compare the FP32 PyTorch model with the OpenVINO INT8 model. These pipelines will also be used for showing the accuracy difference and for benchmarking later in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28ddc37f-5504-4eb1-91c7-063d6f1a8174",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-04T14:42:55.380868Z",
     "iopub.status.busy": "2022-12-04T14:42:55.380738Z",
     "iopub.status.idle": "2022-12-04T14:42:58.064848Z",
     "shell.execute_reply": "2022-12-04T14:42:58.064382Z",
     "shell.execute_reply.started": "2022-12-04T14:42:55.380857Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /tmp/yujiepan/optimum-notebook/textattack/bert-base-uncased-SST-2_JPQD/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/tmp/yujiepan/optimum-notebook/textattack/bert-base-uncased-SST-2_JPQD/config.json\",\n",
      "  \"architectures\": [\n",
      "    \"NNCFNetwork\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"finetuning_task\": \"sst-2\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"positive\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file /tmp/yujiepan/optimum-notebook/textattack/bert-base-uncased-SST-2_FP32/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/tmp/yujiepan/optimum-notebook/textattack/bert-base-uncased-SST-2_FP32\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"finetuning_task\": \"sst-2\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"positive\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file /tmp/yujiepan/optimum-notebook/textattack/bert-base-uncased-SST-2_FP32/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at /tmp/yujiepan/optimum-notebook/textattack/bert-base-uncased-SST-2_FP32.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "optimized_model = OVModelForSequenceClassification.from_pretrained(jpqd_model_path)\n",
    "original_model = AutoModelForSequenceClassification.from_pretrained(fp32_model_path)\n",
    "ov_sst2_pipeline_jpqd = pipeline(\"text-classification\", model=optimized_model, tokenizer=tokenizer)\n",
    "hf_sst2_pipeline = pipeline(\"text-classification\", model=original_model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c885d378-2842-49d0-b583-a2fc023558b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-04T14:42:58.336807Z",
     "iopub.status.busy": "2022-12-04T14:42:58.336478Z",
     "iopub.status.idle": "2022-12-04T14:42:58.495631Z",
     "shell.execute_reply": "2022-12-04T14:42:58.494774Z",
     "shell.execute_reply.started": "2022-12-04T14:42:58.336783Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'positive', 'score': 0.9995125532150269}]\n"
     ]
    }
   ],
   "source": [
    "with patch_tokenizer():\n",
    "    print(ov_sst2_pipeline_jpqd('I love you.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b32fa819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'positive', 'score': 0.9987971782684326}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_sst2_pipeline('I love you.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a52092-e352-47ef-9ed2-89508bc48d70",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "\n",
    "We load the quantized model and the original FP32 model, and compare the metrics on both models. The [evaluate](https://github.com/huggingface/evaluate) library makes it very easy to evaluate models on a given dataset, with a given metric. For the SQuAD dataset, the F1 score and Exact Match metrics are returned.\n",
    "\n",
    "To load the quantized model with OpenVINO, we use the `OVModelForQuestionAnswering` class. It can be used in the same way as [`AutoModelForQuestionAnswering`](https://huggingface.co/docs/transformers/main/model_doc/auto).\n",
    "\n",
    "The pipelines we created in the previous section are used to perform evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c2f615a-19e3-4ee2-9309-2ae1392c7f62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-04T14:42:58.497409Z",
     "iopub.status.busy": "2022-12-04T14:42:58.496740Z",
     "iopub.status.idle": "2022-12-04T14:43:47.884825Z",
     "shell.execute_reply": "2022-12-04T14:43:47.884058Z",
     "shell.execute_reply.started": "2022-12-04T14:42:58.497370Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Disabling tokenizer parallelism, we're using DataLoader multithreading already\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>total_time_in_seconds</th>\n",
       "      <th>samples_per_second</th>\n",
       "      <th>latency_in_seconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>FP32</th>\n",
       "      <td>0.93</td>\n",
       "      <td>30.05</td>\n",
       "      <td>17.04</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>optimized by JPQD</th>\n",
       "      <td>0.92</td>\n",
       "      <td>10.90</td>\n",
       "      <td>46.98</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   accuracy  total_time_in_seconds  samples_per_second  \\\n",
       "FP32                   0.93                  30.05               17.04   \n",
       "optimized by JPQD      0.92                  10.90               46.98   \n",
       "\n",
       "                   latency_in_seconds  \n",
       "FP32                             0.06  \n",
       "optimized by JPQD                0.02  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glue_eval = evaluator(\"text-classification\")\n",
    "input_column = task_to_keys[TASK_NAME][0]\n",
    "\n",
    "with patch_tokenizer():\n",
    "    ov_eval_results = glue_eval.compute(\n",
    "        model_or_pipeline=ov_sst2_pipeline_jpqd,\n",
    "        data=dataset['validation'].select(range(512)),\n",
    "        metric=metric,\n",
    "        input_column=input_column,\n",
    "        label_mapping=label2id,\n",
    "    )\n",
    "    hf_eval_results = glue_eval.compute(\n",
    "        model_or_pipeline=hf_sst2_pipeline,\n",
    "        data=dataset['validation'].select(range(512)),\n",
    "        metric=metric,\n",
    "        input_column=input_column,\n",
    "        label_mapping=label2id,\n",
    "    )\n",
    "\n",
    "pd.DataFrame.from_records(\n",
    "    [hf_eval_results, ov_eval_results],\n",
    "    index=[\"FP32\", \"optimized by JPQD\"],\n",
    ").round(2)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "db183795-6dae-4ef6-847d-042223264149",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-07T21:25:39.912874Z",
     "iopub.status.busy": "2022-11-07T21:25:39.912662Z",
     "iopub.status.idle": "2022-11-07T21:25:39.916029Z",
     "shell.execute_reply": "2022-11-07T21:25:39.915541Z",
     "shell.execute_reply.started": "2022-11-07T21:25:39.912859Z"
    }
   },
   "source": [
    "### Inference Results\n",
    "\n",
    "To fully understand the quality of a model, it is useful to look beyond metrics like Exact Match and F1 score and examine model predictions directly. This can give a more complete impression of the model's performance and help identify areas for improvement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "256e4b8c-a791-4668-95e3-4d23720fcf94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-04T14:43:47.886011Z",
     "iopub.status.busy": "2022-12-04T14:43:47.885656Z",
     "iopub.status.idle": "2022-12-04T14:44:36.260004Z",
     "shell.execute_reply": "2022-12-04T14:44:36.259360Z",
     "shell.execute_reply.started": "2022-12-04T14:43:47.885988Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample</th>\n",
       "      <th>fp32_label</th>\n",
       "      <th>fp32_score</th>\n",
       "      <th>jpqd_label</th>\n",
       "      <th>jpqd_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>it 's a charming and often affecting journey .</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.999766</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.999717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>unflinchingly bleak and desperate</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.985671</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.995319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>allows us to hope that nolan is poised to embark a major career as a commercial yet inventive filmmaker .</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.999574</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.999692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the acting , costumes , music , cinematography and sound are all astounding given the production 's austere locales .</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.996028</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.999448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>it 's slow -- very , very slow .</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.997952</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.997852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>although laced with humor and a few fanciful touches , the film is a refreshingly serious look at young women .</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.999637</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.999698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>a sometimes tedious film .</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.996069</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.996758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>or doing last year 's taxes with your ex-wife .</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.994072</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.995128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>you do n't have to know about music to appreciate the film 's easygoing blend of comedy and romance .</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.999640</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.999723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>in exactly 89 minutes , most of which passed as slowly as if i 'd been sitting naked on an igloo , formula 51 sank from quirky to jerky to utter turkey .</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.997039</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.997929</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                       sample  \\\n",
       "0                                                                                                             it 's a charming and often affecting journey .    \n",
       "1                                                                                                                          unflinchingly bleak and desperate    \n",
       "2                                                  allows us to hope that nolan is poised to embark a major career as a commercial yet inventive filmmaker .    \n",
       "3                                      the acting , costumes , music , cinematography and sound are all astounding given the production 's austere locales .    \n",
       "4                                                                                                                           it 's slow -- very , very slow .    \n",
       "5                                            although laced with humor and a few fanciful touches , the film is a refreshingly serious look at young women .    \n",
       "6                                                                                                                                 a sometimes tedious film .    \n",
       "7                                                                                                            or doing last year 's taxes with your ex-wife .    \n",
       "8                                                      you do n't have to know about music to appreciate the film 's easygoing blend of comedy and romance .    \n",
       "9  in exactly 89 minutes , most of which passed as slowly as if i 'd been sitting naked on an igloo , formula 51 sank from quirky to jerky to utter turkey .    \n",
       "\n",
       "  fp32_label  fp32_score jpqd_label  jpqd_score  \n",
       "0   positive    0.999766   positive    0.999717  \n",
       "1   negative    0.985671   negative    0.995319  \n",
       "2   positive    0.999574   positive    0.999692  \n",
       "3   positive    0.996028   positive    0.999448  \n",
       "4   negative    0.997952   negative    0.997852  \n",
       "5   positive    0.999637   positive    0.999698  \n",
       "6   negative    0.996069   negative    0.996758  \n",
       "7   negative    0.994072   negative    0.995128  \n",
       "8   positive    0.999640   positive    0.999723  \n",
       "9   negative    0.997039   negative    0.997929  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "validation_examples = dataset['validation'].select(range(10))\n",
    "\n",
    "with patch_tokenizer():\n",
    "    for item in validation_examples:\n",
    "        fp32_outputs = hf_sst2_pipeline(item[input_column])[0]\n",
    "        jpqd_outputs = ov_sst2_pipeline_jpqd(item[input_column])[0]\n",
    "        results.append([\n",
    "            item[input_column],\n",
    "            fp32_outputs['label'], fp32_outputs['score'],\n",
    "            jpqd_outputs['label'], jpqd_outputs['score']\n",
    "        ])\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.DataFrame(\n",
    "    results,\n",
    "    columns=['sample','fp32_label', 'fp32_score', 'jpqd_label', 'jpqd_score']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58df445d-af43-4ba1-8195-7d8f00b8f82f",
   "metadata": {},
   "source": [
    "### Model Size\n",
    "\n",
    "We save the FP32 PyTorch model and define a function to show the model size for the PyTorch and OpenVINO models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1eeaa81f-7fc5-49ba-80b8-2d95a1310a0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-04T14:44:36.261080Z",
     "iopub.status.busy": "2022-12-04T14:44:36.260880Z",
     "iopub.status.idle": "2022-12-04T14:44:37.575213Z",
     "shell.execute_reply": "2022-12-04T14:44:37.574836Z",
     "shell.execute_reply.started": "2022-12-04T14:44:36.261063Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 model size: 438.01 MB\n",
      "INT8 model size: 106.14 MB\n",
      "INT8 size decrease: 4.13x\n"
     ]
    }
   ],
   "source": [
    "def get_model_size(model_folder, framework):\n",
    "    \"\"\"\n",
    "    Return OpenVINO or PyTorch model size in Mb. \n",
    "    Arguments: \n",
    "        model_folder: \n",
    "            Directory containing a pytorch_model.bin for a PyTorch model, and an openvino_model.xml/.bin for an OpenVINO model. \n",
    "        framework: \n",
    "            Define whether the model is a PyTorch or an OpenVINO model.\n",
    "    \"\"\"\n",
    "    if framework.lower() == \"openvino\":\n",
    "        model_path = Path(model_folder) / \"openvino_model.xml\"\n",
    "        model_size = model_path.stat().st_size + model_path.with_suffix(\".bin\").stat().st_size\n",
    "    elif framework.lower() == \"pytorch\":\n",
    "        model_path = Path(model_folder) / \"pytorch_model.bin\"\n",
    "        model_size = model_path.stat().st_size\n",
    "    model_size /= 1000 * 1000\n",
    "    return model_size\n",
    "\n",
    "fp32_model_size = get_model_size(fp32_model_path, \"pytorch\")\n",
    "int8_model_size = get_model_size(jpqd_model_path, \"openvino\")\n",
    "print(f\"FP32 model size: {fp32_model_size:.2f} MB\")\n",
    "print(f\"INT8 model size: {int8_model_size:.2f} MB\")\n",
    "print(f\"INT8 size decrease: {fp32_model_size / int8_model_size:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8697a2-0a51-427f-8245-cda56bb8cf18",
   "metadata": {},
   "source": [
    "### Benchmarks\n",
    "\n",
    "Compare the inference speed of the quantized OpenVINO model with that of the original PyTorch model.\n",
    "\n",
    "This benchmark provides an estimate of performance, but keep in mind that other programs running on the computer, as well as power management settings, can affect performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8806da79-0b3b-403e-a40c-61db6a0f482d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-04T14:44:37.575841Z",
     "iopub.status.busy": "2022-12-04T14:44:37.575681Z",
     "iopub.status.idle": "2022-12-04T14:44:53.148782Z",
     "shell.execute_reply": "2022-12-04T14:44:53.147790Z",
     "shell.execute_reply.started": "2022-12-04T14:44:37.575830Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel(R) Xeon(R) Gold 5218 CPU @ 2.30GHz\n",
      "Latency of original FP32 model: 57.60 ms\n",
      "Latency of quantized model: 18.07 ms\n",
      "Speedup: 3.19x\n"
     ]
    }
   ],
   "source": [
    "@patch_tokenizer()\n",
    "def benchmark(pipeline, dataset, num_items=100):\n",
    "    \"\"\"\n",
    "    Benchmark PyTorch or OpenVINO model. This function does inference on `num_items`\n",
    "    dataset items and returns the median latency in milliseconds\n",
    "    \"\"\"\n",
    "    latencies = []\n",
    "    for i, item in enumerate(dataset.select(range(num_items))):\n",
    "        start_time = time.perf_counter()\n",
    "        results = pipeline(item[input_column])\n",
    "        end_time = time.perf_counter()\n",
    "        latencies.append(end_time - start_time)\n",
    "\n",
    "    return np.median(latencies) * 1000\n",
    "\n",
    "\n",
    "original_latency = benchmark(hf_sst2_pipeline, dataset['validation'])\n",
    "quantized_latency = benchmark(ov_sst2_pipeline_jpqd, dataset['validation'])\n",
    "cpu_device_name = Core().get_property(\"CPU\", \"FULL_DEVICE_NAME\")\n",
    "\n",
    "print(cpu_device_name)\n",
    "print(f\"Latency of original FP32 model: {original_latency:.2f} ms\")\n",
    "print(f\"Latency of quantized model: {quantized_latency:.2f} ms\")\n",
    "print(f\"Speedup: {(original_latency/quantized_latency):.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "354bec5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics of the sparsified model:\n",
      "+-----------------------------------------+-------+\n",
      "|            Statistic's name             | Value |\n",
      "+=========================================+=======+\n",
      "| Sparsity level of the whole model       | 0.045 |\n",
      "+-----------------------------------------+-------+\n",
      "| Sparsity level of all sparsified layers | 0.058 |\n",
      "+-----------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stat_lines = trainer.compression_controller.statistics().to_str().split('\\n')\n",
    "print('\\n'.join(stat_lines[:9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547eee08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "c30c450e430028f81574c777bb25e3ac5a02c782df8066b610b99127d866d557"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
